"use strict";
var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {
    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }
    return new (P || (P = Promise))(function (resolve, reject) {
        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }
        function rejected(value) { try { step(generator["throw"](value)); } catch (e) { reject(e); } }
        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }
        step((generator = generator.apply(thisArg, _arguments || [])).next());
    });
};
var __generator = (this && this.__generator) || function (thisArg, body) {
    var _ = { label: 0, sent: function() { if (t[0] & 1) throw t[1]; return t[1]; }, trys: [], ops: [] }, f, y, t, g;
    return g = { next: verb(0), "throw": verb(1), "return": verb(2) }, typeof Symbol === "function" && (g[Symbol.iterator] = function() { return this; }), g;
    function verb(n) { return function (v) { return step([n, v]); }; }
    function step(op) {
        if (f) throw new TypeError("Generator is already executing.");
        while (g && (g = 0, op[0] && (_ = 0)), _) try {
            if (f = 1, y && (t = op[0] & 2 ? y["return"] : op[0] ? y["throw"] || ((t = y["return"]) && t.call(y), 0) : y.next) && !(t = t.call(y, op[1])).done) return t;
            if (y = 0, t) op = [op[0] & 2, t.value];
            switch (op[0]) {
                case 0: case 1: t = op; break;
                case 4: _.label++; return { value: op[1], done: false };
                case 5: _.label++; y = op[1]; op = [0]; continue;
                case 7: op = _.ops.pop(); _.trys.pop(); continue;
                default:
                    if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) { _ = 0; continue; }
                    if (op[0] === 3 && (!t || (op[1] > t[0] && op[1] < t[3]))) { _.label = op[1]; break; }
                    if (op[0] === 6 && _.label < t[1]) { _.label = t[1]; t = op; break; }
                    if (t && _.label < t[2]) { _.label = t[2]; _.ops.push(op); break; }
                    if (t[2]) _.ops.pop();
                    _.trys.pop(); continue;
            }
            op = body.call(thisArg, _);
        } catch (e) { op = [6, e]; y = 0; } finally { f = t = 0; }
        if (op[0] & 5) throw op[1]; return { value: op[0] ? op[1] : void 0, done: true };
    }
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.load = void 0;
/**
 * @license
 * Copyright 2020 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
var tfconv = require("@tensorflow/tfjs-converter");
var tf = require("@tensorflow/tfjs-core");
var bert_tokenizer_1 = require("./bert_tokenizer");
var MODEL_URL = 'https://tfhub.dev/tensorflow/tfjs-model/mobilebert/1';
var INPUT_SIZE = 384;
var MAX_ANSWER_LEN = 32;
var MAX_QUERY_LEN = 64;
var MAX_SEQ_LEN = 384;
var PREDICT_ANSWER_NUM = 5;
var OUTPUT_OFFSET = 1;
// This is threshold value for determining if a question is irrelevant to the
// context. This value comes from the QnA model, and is generated by the
// training process based on the SQUaD 2.0 dataset.
var NO_ANSWER_THRESHOLD = 4.3980759382247925;
var QuestionAndAnswerImpl = /** @class */ (function () {
    function QuestionAndAnswerImpl(modelConfig) {
        this.modelConfig = modelConfig;
        if (this.modelConfig == null) {
            this.modelConfig = { modelUrl: MODEL_URL, fromTFHub: true };
        }
        if (this.modelConfig.fromTFHub == null) {
            this.modelConfig.fromTFHub = false;
        }
    }
    QuestionAndAnswerImpl.prototype.process = function (query, context, maxQueryLen, maxSeqLen, docStride) {
        if (docStride === void 0) { docStride = 128; }
        // always add the question mark to the end of the query.
        query = query.replace(/\?/g, '');
        query = query.trim();
        query = query + '?';
        var queryTokens = this.tokenizer.tokenize(query);
        if (queryTokens.length > maxQueryLen) {
            throw new Error("The length of question token exceeds the limit (".concat(maxQueryLen, ")."));
        }
        var origTokens = this.tokenizer.processInput(context.trim());
        var tokenToOrigIndex = [];
        var allDocTokens = [];
        for (var i = 0; i < origTokens.length; i++) {
            var token = origTokens[i].text;
            var subTokens = this.tokenizer.tokenize(token);
            for (var j = 0; j < subTokens.length; j++) {
                var subToken = subTokens[j];
                tokenToOrigIndex.push(i);
                allDocTokens.push(subToken);
            }
        }
        // The -3 accounts for [CLS], [SEP] and [SEP]
        var maxContextLen = maxSeqLen - queryTokens.length - 3;
        // We can have documents that are longer than the maximum sequence
        // length. To deal with this we do a sliding window approach, where we
        // take chunks of the up to our max length with a stride of
        // `doc_stride`.
        var docSpans = [];
        var startOffset = 0;
        while (startOffset < allDocTokens.length) {
            var length_1 = allDocTokens.length - startOffset;
            if (length_1 > maxContextLen) {
                length_1 = maxContextLen;
            }
            docSpans.push({ start: startOffset, length: length_1 });
            if (startOffset + length_1 === allDocTokens.length) {
                break;
            }
            startOffset += Math.min(length_1, docStride);
        }
        var features = docSpans.map(function (docSpan) {
            var tokens = [];
            var segmentIds = [];
            var tokenToOrigMap = {};
            tokens.push(bert_tokenizer_1.CLS_INDEX);
            segmentIds.push(0);
            for (var i = 0; i < queryTokens.length; i++) {
                var queryToken = queryTokens[i];
                tokens.push(queryToken);
                segmentIds.push(0);
            }
            tokens.push(bert_tokenizer_1.SEP_INDEX);
            segmentIds.push(0);
            for (var i = 0; i < docSpan.length; i++) {
                var splitTokenIndex = i + docSpan.start;
                var docToken = allDocTokens[splitTokenIndex];
                tokens.push(docToken);
                segmentIds.push(1);
                tokenToOrigMap[tokens.length] = tokenToOrigIndex[splitTokenIndex];
            }
            tokens.push(bert_tokenizer_1.SEP_INDEX);
            segmentIds.push(1);
            var inputIds = tokens;
            var inputMask = inputIds.map(function (id) { return 1; });
            while ((inputIds.length < maxSeqLen)) {
                inputIds.push(0);
                inputMask.push(0);
                segmentIds.push(0);
            }
            return { inputIds: inputIds, inputMask: inputMask, segmentIds: segmentIds, origTokens: origTokens, tokenToOrigMap: tokenToOrigMap };
        });
        return features;
    };
    QuestionAndAnswerImpl.prototype.load = function () {
        return __awaiter(this, void 0, void 0, function () {
            var _a, batchSize, inputIds, segmentIds, inputMask, _b;
            return __generator(this, function (_c) {
                switch (_c.label) {
                    case 0:
                        _a = this;
                        return [4 /*yield*/, tfconv.loadGraphModel(this.modelConfig.modelUrl, { fromTFHub: this.modelConfig.fromTFHub })];
                    case 1:
                        _a.model = _c.sent();
                        batchSize = 1;
                        inputIds = tf.ones([batchSize, INPUT_SIZE], 'int32');
                        segmentIds = tf.ones([1, INPUT_SIZE], 'int32');
                        inputMask = tf.ones([1, INPUT_SIZE], 'int32');
                        this.model.execute({
                            input_ids: inputIds,
                            segment_ids: segmentIds,
                            input_mask: inputMask,
                            global_step: tf.scalar(1, 'int32')
                        });
                        _b = this;
                        return [4 /*yield*/, (0, bert_tokenizer_1.loadTokenizer)()];
                    case 2:
                        _b.tokenizer = _c.sent();
                        return [2 /*return*/];
                }
            });
        });
    };
    /**
     * Given the question and context, find the best answers.
     * @param question the question to find answers for.
     * @param context context where the answers are looked up from.
     * @return array of answers
     */
    QuestionAndAnswerImpl.prototype.findAnswers = function (question, context) {
        return __awaiter(this, void 0, void 0, function () {
            var features, inputIdArray, segmentIdArray, inputMaskArray, globalStep, batchSize, result, logits, answers, i;
            var _this = this;
            return __generator(this, function (_a) {
                switch (_a.label) {
                    case 0:
                        if (question == null || context == null) {
                            throw new Error('The input to findAnswers call is null, ' +
                                'please pass a string as input.');
                        }
                        features = this.process(question, context, MAX_QUERY_LEN, MAX_SEQ_LEN);
                        inputIdArray = features.map(function (f) { return f.inputIds; });
                        segmentIdArray = features.map(function (f) { return f.segmentIds; });
                        inputMaskArray = features.map(function (f) { return f.inputMask; });
                        globalStep = tf.scalar(1, 'int32');
                        batchSize = features.length;
                        result = tf.tidy(function () {
                            var inputIds = tf.tensor2d(inputIdArray, [batchSize, INPUT_SIZE], 'int32');
                            var segmentIds = tf.tensor2d(segmentIdArray, [batchSize, INPUT_SIZE], 'int32');
                            var inputMask = tf.tensor2d(inputMaskArray, [batchSize, INPUT_SIZE], 'int32');
                            return _this.model.execute({
                                input_ids: inputIds,
                                segment_ids: segmentIds,
                                input_mask: inputMask,
                                global_step: globalStep
                            }, ['start_logits', 'end_logits']);
                        });
                        return [4 /*yield*/, Promise.all([result[0].array(), result[1].array()])];
                    case 1:
                        logits = _a.sent();
                        // dispose all intermediate tensors
                        globalStep.dispose();
                        result[0].dispose();
                        result[1].dispose();
                        answers = [];
                        for (i = 0; i < batchSize; i++) {
                            answers.push(this.getBestAnswers(logits[0][i], logits[1][i], features[i].origTokens, features[i].tokenToOrigMap, context, i));
                        }
                        return [2 /*return*/, answers.reduce(function (flatten, array) { return flatten.concat(array); }, [])
                                .sort(function (logitA, logitB) { return logitB.score - logitA.score; })
                                .slice(0, PREDICT_ANSWER_NUM)];
                }
            });
        });
    };
    /**
     * Find the Best N answers & logits from the logits array and input feature.
     * @param startLogits start index for the answers
     * @param endLogits end index for the answers
     * @param origTokens original tokens of the passage
     * @param tokenToOrigMap token to index mapping
     */
    QuestionAndAnswerImpl.prototype.getBestAnswers = function (startLogits, endLogits, origTokens, tokenToOrigMap, context, docIndex) {
        var _a;
        if (docIndex === void 0) { docIndex = 0; }
        // Model uses the closed interval [start, end] for indices.
        var startIndexes = this.getBestIndex(startLogits);
        var endIndexes = this.getBestIndex(endLogits);
        var origResults = [];
        startIndexes.forEach(function (start) {
            endIndexes.forEach(function (end) {
                if (tokenToOrigMap[start + OUTPUT_OFFSET] && tokenToOrigMap[end + OUTPUT_OFFSET] && end >= start) {
                    var length_2 = end - start + 1;
                    if (length_2 < MAX_ANSWER_LEN) {
                        origResults.push({ start: start, end: end, score: startLogits[start] + endLogits[end] });
                    }
                }
            });
        });
        origResults.sort(function (a, b) { return b.score - a.score; });
        var answers = [];
        for (var i = 0; i < origResults.length; i++) {
            if (i >= PREDICT_ANSWER_NUM ||
                origResults[i].score < NO_ANSWER_THRESHOLD) {
                break;
            }
            var convertedText = '';
            var startIndex = 0;
            var endIndex = 0;
            if (origResults[i].start > 0) {
                _a = this.convertBack(origTokens, tokenToOrigMap, origResults[i].start, origResults[i].end, context), convertedText = _a[0], startIndex = _a[1], endIndex = _a[2];
            }
            else {
                convertedText = '';
            }
            answers.push({
                text: convertedText,
                score: origResults[i].score,
                startIndex: startIndex,
                endIndex: endIndex
            });
        }
        return answers;
    };
    /** Get the n-best logits from a list of all the logits. */
    QuestionAndAnswerImpl.prototype.getBestIndex = function (logits) {
        var tmpList = [];
        for (var i = 0; i < MAX_SEQ_LEN; i++) {
            tmpList.push([i, i, logits[i]]);
        }
        tmpList.sort(function (a, b) { return b[2] - a[2]; });
        var indexes = [];
        for (var i = 0; i < PREDICT_ANSWER_NUM; i++) {
            indexes.push(tmpList[i][0]);
        }
        return indexes;
    };
    /** Convert the answer back to original text form. */
    QuestionAndAnswerImpl.prototype.convertBack = function (origTokens, tokenToOrigMap, start, end, context) {
        // Shifted index is: index of logits + offset.
        var shiftedStart = start + OUTPUT_OFFSET;
        var shiftedEnd = end + OUTPUT_OFFSET;
        var startIndex = tokenToOrigMap[shiftedStart];
        var endIndex = tokenToOrigMap[shiftedEnd];
        var startCharIndex = origTokens[startIndex].index;
        var endCharIndex = endIndex < origTokens.length - 1 ?
            origTokens[endIndex + 1].index - 1 :
            origTokens[endIndex].index + origTokens[endIndex].text.length;
        return [
            context.slice(startCharIndex, endCharIndex + 1).trim(), startCharIndex,
            endCharIndex
        ];
    };
    return QuestionAndAnswerImpl;
}());
function load(modelConfig) {
    return __awaiter(this, void 0, void 0, function () {
        var mobileBert;
        return __generator(this, function (_a) {
            switch (_a.label) {
                case 0:
                    mobileBert = new QuestionAndAnswerImpl(modelConfig);
                    return [4 /*yield*/, mobileBert.load()];
                case 1:
                    _a.sent();
                    return [2 /*return*/, mobileBert];
            }
        });
    });
}
exports.load = load;
//# sourceMappingURL=question_and_answer.js.map